{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment is available as a publically available docker container: `hamelsmu/ml-gpu`\n",
    "\n",
    "### Pre-requisite: Familiarize yourself with sequence-to-sequence models\n",
    "\n",
    "If you are not familiar with sequence to sequence models, please refer to [this tutorial](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8).\n",
    "\n",
    "### Pre-Requisite: Make Sure you have the right files prepared from Step 1\n",
    "\n",
    "You should have these files in the root of the `./data/processed_data/` directory:\n",
    "\n",
    "1. `{train/valid/test.function}` - these are python function definitions tokenized (by space), 1 line per function.\n",
    "2. `{train/valid/test.docstring}` - these are docstrings that correspond to each of the python function definitions, and have a 1:1 correspondence with the lines in *.function files.\n",
    "3. `{train/valid/test.lineage}` - every line in this file contains a link back to the original location (github repo link) where the code was retrieved.  There is a 1:1 correspondence with the lines in this file and the other two files. This is useful for debugging.\n",
    "\n",
    "\n",
    "### Set the value of `use_cache` appropriately.  \n",
    "\n",
    "If `use_cache = True`, data will be downloaded where possible instead of re-computing.  However, it is highly recommended that you set `use_cache = False` for this tutorial as it will be less confusing, and you will learn more by runing these steps yourself. **This notebook takes approximately 4 hours to run on an AWS `p3.8xlarge` instance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: you can set what GPU you want to use in a notebook like this.  \n",
    "# # Useful if you want to run concurrent experiments at the same time on different GPUs.\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# This will allow the notebook to run faster\n",
    "from pathlib import Path\n",
    "from general_utils import get_step2_prerequisite_files, read_training_files\n",
    "from keras.utils import get_file\n",
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "import logging\n",
    "import pandas as pd\n",
    "OUTPUT_PATH = Path('./data/seq2seq/')\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Text From File\n",
    "\n",
    "We want to read in raw text from files so we can pre-process the text for modeling as described in [this tutorial](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Num rows for encoder training input: 31,765\n",
      "WARNING:root:Num rows for encoder validation input: 649\n",
      "WARNING:root:Num rows for encoder holdout input: 328\n",
      "WARNING:root:Num rows for encoder training input: 31,765\n",
      "WARNING:root:Num rows for encoder validation input: 649\n",
      "WARNING:root:Num rows for encoder holdout input: 328\n"
     ]
    }
   ],
   "source": [
    "if use_cache:\n",
    "    get_step2_prerequisite_files(output_directory = './data/processed_data')\n",
    "def read_training_files(data_path:str):\n",
    "    \"\"\"\n",
    "    Read data from directory\n",
    "    \"\"\"\n",
    "    PATH = Path(data_path)\n",
    "\n",
    "    with open(PATH/'train.function', 'r') as f:\n",
    "        t_enc = f.readlines()\n",
    "        \n",
    "    with open(PATH/'valid.function', 'r') as f:\n",
    "        v_enc = f.readlines()\n",
    "\n",
    "    with open(PATH/'test.function', 'r') as f:\n",
    "        h_enc = f.readlines()\n",
    "\n",
    "    with open(PATH/'train.docstring', 'r') as f:\n",
    "        t_dec = f.readlines()\n",
    "\n",
    "    with open(PATH/'valid.docstring', 'r') as f:\n",
    "        v_dec = f.readlines()\n",
    "\n",
    "    with open(PATH/'test.docstring', 'r') as f:\n",
    "        h_dec = f.readlines()\n",
    "\n",
    "    logging.warning(f'Num rows for encoder training input: {len(t_enc):,}')\n",
    "    logging.warning(f'Num rows for encoder validation input: {len(v_enc):,}')\n",
    "    logging.warning(f'Num rows for encoder holdout input: {len(h_enc):,}')\n",
    "\n",
    "    logging.warning(f'Num rows for encoder training input: {len(t_dec):,}')\n",
    "    logging.warning(f'Num rows for encoder validation input: {len(v_dec):,}')\n",
    "    logging.warning(f'Num rows for encoder holdout input: {len(h_dec):,}')\n",
    "\n",
    "    return t_enc, v_enc, h_enc, t_dec, v_dec, h_dec\n",
    "# you want to supply the directory where the files are from step 1.\n",
    "train_code, valid_code, holdout_code, train_comment, valid_comment, holdout_comment = read_training_files('./data/processed_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code and comment files should be of the same length.\n",
    "\n",
    "assert len(train_code) == len(train_comment)\n",
    "assert len(valid_code) == len(valid_comment)\n",
    "assert len(holdout_code) == len(holdout_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Text\n",
    "\n",
    "In this step, we are going to pre-process the raw text for modeling.  For an explanation of what this section does, see the [Preapre & Clean Data section of this Tutorial](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 55 based upon hueristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 4 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 0 sec\n",
      "WARNING:root:Finished parsing 31,765 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 0 sec\n",
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 15 based upon hueristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 1 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 0 sec\n",
      "WARNING:root:Finished parsing 31,765 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 0 sec\n"
     ]
    }
   ],
   "source": [
    "from ktext.preprocess import processor\n",
    "\n",
    "if not use_cache:    \n",
    "    code_proc = processor(hueristic_pct_padding=.7, keep_n=20000)\n",
    "    t_code = code_proc.fit_transform(train_code)\n",
    "\n",
    "    comment_proc = processor(append_indicators=True, hueristic_pct_padding=.7, keep_n=14000, padding ='post')\n",
    "    t_comment = comment_proc.fit_transform(train_comment)\n",
    "\n",
    "elif use_cache:\n",
    "    logging.warning('Not fitting transform function because use_cache=True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save tokenized text** (You will reuse this for step 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "if not use_cache:\n",
    "    # Save the preprocessor\n",
    "    with open(OUTPUT_PATH/'py_code_proc_v2.dpkl', 'wb') as f:\n",
    "        dpickle.dump(code_proc, f)\n",
    "\n",
    "    with open(OUTPUT_PATH/'py_comment_proc_v2.dpkl', 'wb') as f:\n",
    "        dpickle.dump(comment_proc, f)\n",
    "\n",
    "    # Save the processed data\n",
    "    np.save(OUTPUT_PATH/'py_t_code_vecs_v2.npy', t_code)\n",
    "    np.save(OUTPUT_PATH/'py_t_comment_vecs_v2.npy', t_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrange data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (31765, 55)\n",
      "Shape of decoder input: (31765, 14)\n",
      "Shape of decoder target: (31765, 14)\n",
      "Size of vocabulary for data/seq2seq/py_code_proc_v2.dpkl: 20,002\n",
      "Size of vocabulary for data/seq2seq/py_comment_proc_v2.dpkl: 14,002\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor\n",
    "\n",
    "\n",
    "encoder_input_data, encoder_seq_len = load_encoder_inputs(OUTPUT_PATH/'py_t_code_vecs_v2.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs(OUTPUT_PATH/'py_t_comment_vecs_v2.npy')\n",
    "num_encoder_tokens, enc_pp = load_text_processor(OUTPUT_PATH/'py_code_proc_v2.dpkl')\n",
    "num_decoder_tokens, dec_pp = load_text_processor(OUTPUT_PATH/'py_comment_proc_v2.dpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have the above files on disk because you set `use_cache = True` you can download the files for the above function calls here:\n",
    "\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_t_code_vecs_v2.npy\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_t_comment_vecs_v2.npy\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_code_proc_v2.dpkl\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_comment_proc_v2.dpkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Seq2Seq Model For Summarizing Code\n",
    "\n",
    "We will build a model to predict the docstring given a function or a method.  While this is a very cool task in itself, this is not the end goal of this exercise.  The motivation for training this model is to learn a general purpose feature extractor for code that we can use for the task of code search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import build_seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convenience function `build_seq2seq_model` constructs the architecture for a sequence-to-sequence model.  \n",
    "\n",
    "The architecture built for this tutorial is a minimal example with only one layer for the encoder and decoder, and does not include things like [attention](https://nlp.stanford.edu/pubs/emnlp15_attn.pdf).  We encourage you to try and build different architectures to see what works best for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_Model = build_seq2seq_model(word_emb_dim=800,\n",
    "                                    hidden_state_dim=1000,\n",
    "                                    encoder_seq_len=encoder_seq_len,\n",
    "                                    num_encoder_tokens=num_encoder_tokens,\n",
    "                                    num_decoder_tokens=num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 800)    11201600    Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 55)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 800)    3200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 1000)         21407800    Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 1000), 5403000     Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 1000)   4000        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 14002)  14016002    Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 52,035,602\n",
      "Trainable params: 52,030,402\n",
      "Non-trainable params: 5,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "31765/31765 [==============================] - 94s 3ms/step - loss: 7.3149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Generating predictions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f010421b884611b0c2aeb1c1f6d0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=328), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Calculating BLEU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.035637027120182886\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31765/31765 [==============================] - 88s 3ms/step - loss: 5.0426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Generating predictions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba5b082df2746dc9a40e3cfeb11a3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=328), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Calculating BLEU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.035637027120182886\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import Callback\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "def evaluate_model(model):\n",
    "    seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=enc_pp,\n",
    "                                 decoder_preprocessor=dec_pp,\n",
    "                                 seq2seq_model=model)\n",
    "    return seq2seq_inf.evaluate_model(input_strings=holdout_code, \n",
    "                           output_strings=holdout_comment, \n",
    "                           max_len=None)\n",
    "\n",
    "class EvaluateCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        bleu_score = evaluate_model(self.model)\n",
    "        print(bleu_score)\n",
    "    \n",
    "if not use_cache:\n",
    "\n",
    "    from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "    import numpy as np\n",
    "    from keras import optimizers\n",
    "\n",
    "    seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.00015), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "    batch_size = 800\n",
    "    epochs = 16\n",
    "    evaluateCallback = EvaluateCallback()\n",
    "    history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks = [evaluateCallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cache:\n",
    "    logging.warning('Not re-training function summarizer seq2seq model because use_cache=True')\n",
    "    # Load model from url\n",
    "    loc = get_file(fname='py_func_sum_v9_.epoch16-val2.55276.hdf5',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_func_sum_v9_.epoch16-val2.55276.hdf5')\n",
    "    seq2seq_Model = load_model(loc)\n",
    "    \n",
    "    # Load encoder (code) pre-processor from url\n",
    "    loc = get_file(fname='py_code_proc_v2.dpkl',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_code_proc_v2.dpkl')\n",
    "    num_encoder_tokens, enc_pp = load_text_processor(loc)\n",
    "    \n",
    "    # Load decoder (docstrings/comments) pre-processor from url\n",
    "    loc = get_file(fname='py_comment_proc_v2.dpkl',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_comment_proc_v2.dpkl')\n",
    "    num_decoder_tokens, dec_pp = load_text_processor(loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above procedure will automatically download a pre-trained model and associated artifacts from https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/ if `use_cache = True`.  \n",
    "\n",
    "Otherwise, the above code will checkpoint the best model after each epoch into the current directory with prefix `py_func_sum_v9_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Seq2Seq Model For Code Summarization\n",
    "\n",
    "To evaluate this model we are going to do two things:\n",
    "\n",
    "1.  Manually inspect the results of predicted docstrings for code snippets, to make sure they look sensible.\n",
    "2.  Calculate the [BLEU Score](https://en.wikipedia.org/wiki/BLEU) so that we can quantitately benchmark different iterations of this algorithm and to guide hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Inspect Results (on holdout set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 278 =================\n",
      "\n",
      "Original Input:\n",
      " function jQuery Controller use strict var PageController Controller extend sap ui core sample ControlBusyIndicator Page onAction function onAction oEvt var oPanel this byId panel1 oPanel setBusy true var oIcon this byId panel2 icon oIcon setBusy true setTimeout function oPanel setBusy false oIcon setBusy false\n",
      " \n",
      "\n",
      "Original Output:\n",
      " simulate delayed end operation\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " pure\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 292 =================\n",
      "\n",
      "Original Input:\n",
      " function handleManifestReloadEvent event if event eventStream value MPD_RELOAD_VALUE var timescale event eventStream timescale 1 var validUntil event presentationTime timescale var newDuration void 0 if event presentationTime 0xFFFFFFFF newDuration NaN else newDuration event presentationTime event duration timescale logger info Manifest validity changed Valid until validUntil remaining duration newDuration eventBus trigger _Events2 default MANIFEST_VALIDITY_CHANGED id event id validUntil validUntil newDuration newDuration newManifestValidAfter NaN\n",
      " \n",
      "\n",
      "Original Output:\n",
      " 0xff ... means remaining duration unknown\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " pure\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 195 =================\n",
      "\n",
      "Original Input:\n",
      " function name if name undefined return name replace a z_ a z_0 9 i 1 return undefined\n",
      " \n",
      "\n",
      "Original Output:\n",
      " @param string name symbol 's longname @return string symbol 's basename\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " pure\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 152 =================\n",
      "\n",
      "Original Input:\n",
      " function finalhandler req res options var opts options var env opts env process env NODE_ENV development var onerror opts onerror return function err var status res statusCode if err res _header debug cannot 404 after headers sent return if err if err statusCode status err statusCode if err status status err status if status status 400 status 500 var msg env production http STATUS_CODES status err stack err toString msg escapeHtml msg replace n g br replace g nbsp n else status 404 msg Cannot escapeHtml req method escapeHtml req originalUrl req url n debug default s status if err onerror defer onerror err req res if res _header return req socket destroy send req res status msg\n",
      " \n",
      "\n",
      "Original Output:\n",
      " create function handle final response @param request req @param response res @param\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " pure\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 22 =================\n",
      "\n",
      "Original Input:\n",
      " function tags return new QueryTag tags this\n",
      " \n",
      "\n",
      "Original Output:\n",
      " perform query given tags returning querytag instance @param string tags @api public\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " pure\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 135 =================\n",
      "\n",
      "Original Input:\n",
      " function test var errs errs push new Error first err errs push new Error second err errs push new Error third err var promise1 Q reject errs 0 var promise2 promise1 fail function e if e errs 0 throw errs 1\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test several fails chaining\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " pure\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 302 =================\n",
      "\n",
      "Original Input:\n",
      " function var _ref _asyncToGenerator regeneratorRuntime mark function _callee return regeneratorRuntime wrap function _callee _context while 1 switch _context prev _context next case 0 this _cli clearScreen _context next 3 return this _cli inquire main case 3 this _selectedOption _context sent _context next 6 return this displayMainMenu case 6 case end return _context stop\n",
      " \n",
      "\n",
      "Original Output:\n",
      " _ _ pure _ _\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " pure\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 321 =================\n",
      "\n",
      "Original Input:\n",
      " function var oThis new DeclarativeSupport sap ui getCore registerPlugin oThis\n",
      " \n",
      "\n",
      "Original Output:\n",
      " create < code > sap.ui.core.plugin declarativesupport < code > plugin register within\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " pure\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 133 =================\n",
      "\n",
      "Original Input:\n",
      " function test var seed I m a seed var sr1 new SeededRandom seed var sr2 new SeededRandom seed var sr1vals var sr2vals for var i 0 i 100 i sr1vals push sr1 next sr2vals push sr2 next test equal sr1vals sr2vals\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test two seeded prngs seed produce values\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " pure\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 285 =================\n",
      "\n",
      "Original Input:\n",
      " function adjustRegexLiteral key value use strict if key value value instanceof RegExp value value toString return value\n",
      " \n",
      "\n",
      "Original Output:\n",
      " special handling regular expression literal since need convert string literal otherwise decoded\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " pure\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 35 =================\n",
      "\n",
      "Original Input:\n",
      " function function fn param1 param2 use strict key1 key2 value var foo bar\n",
      " \n",
      "\n",
      "Original Output:\n",
      " jshint ignore start jscs disable jshint ignore start jscs disable\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " pure\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 49 =================\n",
      "\n",
      "Original Input:\n",
      " function once return this times 1\n",
      " \n",
      "\n",
      "Original Output:\n",
      " sugar syntax times(1 @name @see @link times @public @example nock('http zombo.com).get .once.reply(200\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " pure\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 289 =================\n",
      "\n",
      "Original Input:\n",
      " function filterStateToApiQuery filter var aggregate 0 _lodash isUndefined filter aggregate true filter aggregate return Object assign aggregate aggregate filter action action filter action filter on on filter on filter after after filter after filter before before filter before filter by by filter by filter dateRange date_range filter dateRange filter group group filter group filter notGroup not_group filter notGroup filter name name filter name number 1000\n",
      " \n",
      "\n",
      "Original Output:\n",
      " default 'll tell api create aggregate events\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " pure\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 208 =================\n",
      "\n",
      "Original Input:\n",
      " function templatesDirectory scope try fs accessSync path resolve __dirname templates scope args tpl fs constants R_OK fs constants W_OK return path resolve __dirname templates scope args tpl catch e return path resolve __dirname templates mongoose\n",
      " \n",
      "\n",
      "Original Output:\n",
      " default template mongoose\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " pure\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 289 =================\n",
      "\n",
      "Original Input:\n",
      " function filterStateToApiQuery filter var aggregate 0 _lodash isUndefined filter aggregate true filter aggregate return Object assign aggregate aggregate filter action action filter action filter on on filter on filter after after filter after filter before before filter before filter by by filter by filter dateRange date_range filter dateRange filter group group filter group filter notGroup not_group filter notGroup filter name name filter name number 1000\n",
      " \n",
      "\n",
      "Original Output:\n",
      " default 'll tell api create aggregate events\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " pure\n"
     ]
    }
   ],
   "source": [
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=enc_pp,\n",
    "                                 decoder_preprocessor=dec_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)\n",
    "\n",
    "demo_testdf = pd.DataFrame({'code':holdout_code, 'comment':holdout_comment, 'ref':''})\n",
    "seq2seq_inf.demo_model_predictions(n=15, df=demo_testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on manual inspection of results:\n",
    "\n",
    "The predicted code summaries are not perfect, but we can see that the model has learned to extract some semantic meaning from the code.  That's all we need to get reasonable results in this case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate BLEU Score (on holdout set)\n",
    "\n",
    "BLEU Score is described [in this wikipedia article](https://en.wikipedia.org/wiki/BLEU), and is a way to measure the efficacy of summarization/translation such as the one we conducted here.  This metric is useful if you wish to conduct extensive hyper-parameter tuning and try to improve the seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Generating predictions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7543cb8ece694d98abf2df8e348ae3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=328), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Calculating BLEU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.035637027120182886"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will return a BLEU Score\n",
    "seq2seq_inf.evaluate_model(input_strings=holdout_code, \n",
    "                           output_strings=holdout_comment, \n",
    "                           max_len=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model to disk\n",
    "\n",
    "Save the model to disk so you can use it in Step 4 of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_3:0' shape=(?, 1000) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model.save(OUTPUT_PATH/'code_summary_seq2seq_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free\n"
     ]
    }
   ],
   "source": [
    "from seq2seq_utils import free_gpu_mem\n",
    "free_gpu_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
